{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import PIL.Image\n",
    "from IPython.display import display\n",
    "import math\n",
    "import os\n",
    "import scipy.misc\n",
    "from scipy.stats import itemfreq\n",
    "from random import sample\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataBase_creator(archivezip, nwigth, nheight, save_name):\n",
    "    \n",
    "    s = (len(archivezip.namelist()[:])-1, nwigth, nheight,3) \n",
    "    allImage = np.zeros(s)\n",
    "\n",
    "    for i in range(1,len(archivezip.namelist()[:])):\n",
    "        filename = BytesIO(archivezip.read(archivezip.namelist()[i]))\n",
    "        image = PIL.Image.open(filename) \n",
    "        image = image.resize((nwigth, nheight))\n",
    "        image = np.array(image)\n",
    "        image = np.clip(image/255.0, 0.0, 1.0) \n",
    "\n",
    "        allImage[i-1]=image\n",
    "\n",
    "    pickle.dump(allImage, open( save_name + '.p', \"wb\" ) )\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_csv(\"/home/jash/Desktop/capstone/labels.csv\")\n",
    "y_train=pd.get_dummies(y_train[\"breed\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 40, 40, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_train = ZipFile(\"/home/jash/Desktop/capstone/train.zip\", 'r')\n",
    "image_resize = 40\n",
    "DataBase_creator(archivezip = archive_train, nwigth = image_resize, nheight = image_resize , save_name = \"x_train\")\n",
    "x_train = pickle.load( open( \"x_train.p\", \"rb\" ) )\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x,y,start,batch_size):\n",
    "    x_batch=x[start:start+batch_size]\n",
    "    y_batch=y[start:start+batch_size]\n",
    "    start=start+batch_size\n",
    "    return x_batch,y_batch,start\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    w = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(w)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    b = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(b)\n",
    "\n",
    "\n",
    "\n",
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n",
    " \n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    " \n",
    "    weights = weight_variable(shape)\n",
    "    biases = bias_variable([num_filters])\n",
    "\n",
    "    layer = tf.nn.relu(tf.nn.conv2d(input=input,\n",
    "                                    filter=weights,\n",
    "                                    strides=[1, 2, 2, 1],\n",
    "                                    padding='SAME') + biases)\n",
    "\n",
    "    if use_pooling: \n",
    "        return max_pool(layer), weights\n",
    "\n",
    "        \n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input, num_inputs, num_outputs, use_relu=True): \n",
    "    weights = weight_variable([num_inputs, num_outputs])\n",
    "    biases = bias_variable([num_outputs])\n",
    " \n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    " \n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-6c8b2d400c28>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, image_resize,image_resize,3], name='input_data')\n",
    "x_image = tf.reshape(x, [-1,image_resize,image_resize,3])\n",
    "# correct labels\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 120], name='correct_labels')\n",
    "\n",
    "# fist conv layer\n",
    "convlayer1, w1 = new_conv_layer(x_image, 3, 3, 30)\n",
    "# second conv layer\n",
    "convlayer2, w2 = new_conv_layer(convlayer1, 30, 3, 50)\n",
    "#third conv layer\n",
    "\n",
    "convlayer3,w3=new_conv_layer(convlayer2,50,3,30)\n",
    "\n",
    "# flat layer\n",
    "flat_layer, num_features = flatten_layer(convlayer2)\n",
    "# fully connected layer\n",
    "fclayer = new_fc_layer(flat_layer, num_features, 1024)\n",
    "\n",
    "\n",
    "# DROPOUT\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "drop_layer = tf.nn.dropout(fclayer, keep_prob)\n",
    "# final layer\n",
    "W_f = weight_variable([1024, 120])\n",
    "b_f = bias_variable([120])\n",
    "y_f = tf.matmul(drop_layer, W_f) + b_f\n",
    "y_f_softmax = tf.nn.softmax(y_f)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_f))\n",
    "\n",
    "# train step\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_f_softmax, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training accuracy 0.015000\n",
      "Epoch 1, training accuracy 0.015000\n",
      "Epoch 2, training accuracy 0.005000\n",
      "Epoch 3, training accuracy 0.010000\n",
      "Epoch 4, training accuracy 0.005000\n",
      "Epoch 5, training accuracy 0.035000\n",
      "Epoch 6, training accuracy 0.035000\n",
      "Epoch 7, training accuracy 0.040000\n",
      "Epoch 8, training accuracy 0.035000\n",
      "Epoch 9, training accuracy 0.035000\n",
      "Epoch 10, training accuracy 0.050000\n",
      "Epoch 11, training accuracy 0.065000\n",
      "Epoch 12, training accuracy 0.065000\n",
      "Epoch 13, training accuracy 0.075000\n",
      "Epoch 14, training accuracy 0.075000\n",
      "Epoch 15, training accuracy 0.085000\n",
      "Epoch 16, training accuracy 0.085000\n",
      "Epoch 17, training accuracy 0.090000\n",
      "Epoch 18, training accuracy 0.090000\n",
      "Epoch 19, training accuracy 0.090000\n",
      "Epoch 20, training accuracy 0.100000\n",
      "Epoch 21, training accuracy 0.095000\n",
      "Epoch 22, training accuracy 0.100000\n",
      "Epoch 23, training accuracy 0.105000\n",
      "Epoch 24, training accuracy 0.100000\n",
      "Epoch 25, training accuracy 0.110000\n",
      "Epoch 26, training accuracy 0.105000\n",
      "Epoch 27, training accuracy 0.105000\n",
      "Epoch 28, training accuracy 0.115000\n",
      "Epoch 29, training accuracy 0.105000\n",
      "Epoch 30, training accuracy 0.100000\n",
      "Epoch 31, training accuracy 0.115000\n",
      "Epoch 32, training accuracy 0.135000\n",
      "Epoch 33, training accuracy 0.125000\n",
      "Epoch 34, training accuracy 0.120000\n",
      "Epoch 35, training accuracy 0.125000\n",
      "Epoch 36, training accuracy 0.140000\n",
      "Epoch 37, training accuracy 0.135000\n",
      "Epoch 38, training accuracy 0.120000\n",
      "Epoch 39, training accuracy 0.135000\n",
      "Epoch 40, training accuracy 0.130000\n",
      "Epoch 41, training accuracy 0.125000\n",
      "Epoch 42, training accuracy 0.130000\n",
      "Epoch 43, training accuracy 0.110000\n",
      "Epoch 44, training accuracy 0.135000\n",
      "Epoch 45, training accuracy 0.160000\n",
      "Epoch 46, training accuracy 0.145000\n",
      "Epoch 47, training accuracy 0.135000\n",
      "Epoch 48, training accuracy 0.135000\n",
      "Epoch 49, training accuracy 0.150000\n",
      "0: test accuracy 0.025000\n",
      "avg test accuracy: 0.043999999202787875\n"
     ]
    }
   ],
   "source": [
    "num_steps =40\n",
    "batch_size =200\n",
    "test_size =10\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        start=0\n",
    "        for step in range(num_steps):\n",
    "            x_batch,y_batch,start= batch(x_train,y_train,start,batch_size)\n",
    "         \n",
    "        \n",
    "            if step % 10 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            \n",
    "           \n",
    "            train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "        print('Epoch %d, training accuracy %f' %(epoch, train_accuracy))\n",
    "   \n",
    "    test_accuracy = 0.0\n",
    "    start=0\n",
    "    for i in xrange(test_size):\n",
    "        x_batch,y_batch,start= batch(x_test,y_test,start,batch_size)\n",
    "        acc = accuracy.eval(feed_dict={x: x_batch, y_:y_batch, keep_prob: 1.0})\n",
    "        if i % 10 == 0:\n",
    "            print('%d: test accuracy %f' % (i, acc))\n",
    "        test_accuracy += acc\n",
    "    print 'avg test accuracy:', test_accuracy/(test_size)\n",
    "    file_writer = tf.summary.FileWriter('/home/jash/Desktop/capstone', sess.graph)\n",
    "    file_writer.add_graph(sess.graph)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
